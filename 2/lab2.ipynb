{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "774287925a66145d73668b6a683ac064",
     "grade": false,
     "grade_id": "cell-8d856208da5d0763",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    " # Lab 2: Classification\n",
    "\n",
    "### Machine Learning 1, October 2023\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in this IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact your teaching assistant.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* When practising with this lab you should vectorized your code (and rely on loops as little as possible). Therefore for some questions, we give you a maximum number of loops that are necessary for an efficient implementation. This number refers to the loops in this particular function and does not count the ones in functions that are called from the function. You should not go above this number for the maximum number of points.\n",
    "* Ignore the np_conda warning!\n",
    "* Do not add or delete cells. It can lead to penalties during grading.\n",
    "* If you use google collab, just copy paste your solution to this notebook before the submission. Sometimes, google colab can break the auto-grading mechanism.\n",
    "\n",
    "**IMPORTANT: Note that for this code exercise we are using flipped notation (denominator layout) for the gradient and the derivations.**\n",
    "\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\ba}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bq}{\\mathbf{q}}$\n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bT}{\\mathbf{T}}$\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:40:06.183304392Z",
     "start_time": "2023-09-26T12:40:06.171739634Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d9c5a44d13bdc7545f1a15d6dc9c8c",
     "grade": false,
     "grade_id": "cell-422dbc02437671ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:40:06.576683569Z",
     "start_time": "2023-09-26T12:40:06.559737815Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "902185d2dda7e356189a57a09a637182",
     "grade": false,
     "grade_id": "cell-7f215df0e22ae748",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell makes sure that you have all the necessary libraries installed\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from importlib.util import find_spec, module_from_spec\n",
    "\n",
    "def check_newer_version(version_inst, version_nec):\n",
    "    version_inst_split = version_inst.split('.')\n",
    "    version_nec_split = version_nec.split('.')\n",
    "    for i in range(min(len(version_inst_split), len(version_nec_split))):\n",
    "        if int(version_nec_split[i]) > int(version_inst_split[i]):\n",
    "            return False\n",
    "        elif int(version_nec_split[i]) < int(version_inst_split[i]):\n",
    "            return True\n",
    "    return True\n",
    "        \n",
    "    \n",
    "module_list = [('jupyter', '1.0.0'), \n",
    "               ('matplotlib', '2.0.2'), \n",
    "               ('numpy', '1.13.1'), \n",
    "               ('python', '3.6.2'), \n",
    "               ('sklearn', '0.19.0'), \n",
    "               ('scipy', '0.19.1'), \n",
    "               ('nb_conda', '2.2.1')]\n",
    "\n",
    "packages_correct = True\n",
    "packages_errors = []\n",
    "\n",
    "for module_name, version in module_list:\n",
    "    if module_name == 'scikit-learn':\n",
    "        module_name = 'sklearn'\n",
    "    if module_name == 'pyyaml':\n",
    "        module_name = 'yaml'\n",
    "    if 'python' in module_name:\n",
    "        python_version = platform.python_version()\n",
    "        if not check_newer_version(python_version, version):\n",
    "            packages_correct = False\n",
    "            error = f'Update {module_name} to version {version}. Current version is {python_version}.'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "    else:\n",
    "        spec = find_spec(module_name)\n",
    "        if spec is None:\n",
    "            packages_correct = False\n",
    "            error = f'Install {module_name} with version {version} or newer, it is required for this assignment!'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "        else:\n",
    "            x =__import__(module_name)\n",
    "            if hasattr(x, '__version__') and not check_newer_version(x.__version__, version):\n",
    "                packages_correct = False\n",
    "                error = f'Update {module_name} to version {version}. Current version is {x.__version__}.'\n",
    "                packages_errors.append(error) \n",
    "                print(error)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    packages_correct = False\n",
    "    error = \"\"\"Please, don't use google colab!\n",
    "It will make it much more complicated for us to check your homework as it merges all the cells into one.\"\"\"\n",
    "    packages_errors.append(error) \n",
    "    print(error)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "packages_errors = '\\n'.join(packages_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf70ed5d8eb97c2b1256a7cebb3d6e28",
     "grade": false,
     "grade_id": "cell-821f67d8cd14e4f7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part 1. Multiclass logistic regression\n",
    "\n",
    "You have a friend with one big problem which needs your help: Your friend wants to develop a mobile phone app that can do _machine vision_ using the mobile camera: converting a picture (from the camera) to the meaning of the image. You decide to start with an app that can read handwritten digits, i.e. convert an image of handwritten digits to text (e.g. it would enable her to read precious handwritten phone numbers) by using Logistic Regression.\n",
    "\n",
    "A key building block for such an app would be a function `predict_digit(x)` that returns the digit class of an image patch $\\bx$. Since hand-coding this function is highly non-trivial, you decide to solve this problem using machine learning, such that the internal parameters of this function are automatically learned using machine learning techniques.\n",
    "\n",
    "The dataset you're going to use for this is the MNIST handwritten digits dataset (`http://yann.lecun.com/exdb/mnist/`). You can download the data with scikit learn, and load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:41:05.908593764Z",
     "start_time": "2023-09-26T12:40:07.506360674Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2da19d44d1fd53e4acd2e1629602dd2c",
     "grade": false,
     "grade_id": "cell-bcdbc957165abae7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "# Fetch the data\n",
    "try:\n",
    "    mnist = fetch_openml('mnist_784')\n",
    "except Exception:\n",
    "    raise FileNotFoundError('Please download mnist-original.mat from Canvas and put it in %s/mldata' % os.getcwd())\n",
    "\n",
    "data, target = mnist.data, mnist.target.astype('int')\n",
    "\n",
    "data = data.to_numpy()\n",
    "target = target.to_numpy()\n",
    "# Shuffle\n",
    "\n",
    "\n",
    "indices = np.arange(len(data))\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data, target = data[indices].astype('float32'), target[indices]\n",
    "\n",
    "# Normalize the data between 0.0 and 1.0:\n",
    "data /= 255. \n",
    "\n",
    "# Split\n",
    "x_train, x_valid, x_test = data[:50000], data[50000:60000], data[60000: 70000]\n",
    "t_train, t_valid, t_test = target[:50000], target[50000:60000], target[60000: 70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:41:05.926055004Z",
     "start_time": "2023-09-26T12:41:05.912214659Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c9811210c81479760427f7b18e3caf4",
     "grade": false,
     "grade_id": "cell-7f720174680c886d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b20138af0810741223d2c2ddc82bf0f",
     "grade": false,
     "grade_id": "cell-b7b4a5a96dccf229",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "MNIST consists of small 28 by 28 pixel images of written digits (0-9). We split the dataset into a training, validation and testing arrays. The variables `x_train`, `x_valid` and `x_test` are $N \\times M$ matrices, where $N$ is the number of datapoints in the respective set, and $M = 28^2 = 784$ is the dimensionality of the data. The second set of variables `t_train`, `t_valid` and `t_test` contain the corresponding $N$-dimensional vector of integers, containing the true class labels.\n",
    "\n",
    "Here's a visualisation of the first 8 digits of the trainingset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:41:06.682912086Z",
     "start_time": "2023-09-26T12:41:05.933361921Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e1628ec56b6d664edf9aaf496ea637",
     "grade": false,
     "grade_id": "cell-48a92c0a2a2bf4dd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_digits(data, num_cols, targets=None, shape=(28,28)):\n",
    "    num_digits = data.shape[0]\n",
    "    num_rows = int(num_digits/num_cols)\n",
    "    for i in range(num_digits):\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.imshow(data[i].reshape(shape), interpolation='none', cmap='Greys')\n",
    "        if targets is not None:\n",
    "            plt.title(int(targets[i]))\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_digits(x_train[0:40000:5000], num_cols=4, targets=t_train[0:40000:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7c785a4204a8393a15089c8f770b6c2",
     "grade": false,
     "grade_id": "cell-3eb664a58e03bf42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In _multiclass_ logistic regression, the conditional probability of class label $j$ given the image $\\bx$ for some datapoint is given by:\n",
    "\n",
    "$ \\log p(t = j \\;|\\; \\bx, \\bb, \\bW) = \\log q_j - \\log Z$\n",
    "\n",
    "where $\\log q_j = \\bw_j^T \\bx + b_j$ (the log of the unnormalized probability of the class $j$), and $Z = \\sum_k q_k$ is the normalizing factor. $\\bw_j$ is the $j$-th column of $\\bW$ (a matrix of size $784 \\times 10$) corresponding to the class label, $b_j$ is the $j$-th element of $\\bb$.\n",
    "\n",
    "Given an input image, the multiclass logistic regression model first computes the intermediate vector $\\log \\bq$ (of size $10 \\times 1$), using $\\log q_j = \\bw_j^T \\bx + b_j$, containing the unnormalized log-probabilities per class. \n",
    "\n",
    "The unnormalized probabilities are then normalized by $Z$ such that $\\sum_j p_j = \\sum_j \\exp(\\log p_j) = 1$. This is done by $\\log p_j = \\log q_j - \\log Z$ where $Z = \\sum_i \\exp(\\log q_i)$. This is known as the _softmax_ transformation, and is also used as a last layer of many classifcation neural network models, to ensure that the output of the network is a normalized distribution, regardless of the values of second-to-last layer ($\\log \\bq$)\n",
    "\n",
    "**Warning**: when computing $\\log Z$, you are likely to encounter numerical problems. Save yourself countless hours of debugging and learn the [log-sum-exp trick](https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/ \"Title\").\n",
    "\n",
    "The network's output $\\log \\bp$ of size $10 \\times 1$ then contains the conditional log-probabilities $\\log p(t = j \\;|\\; \\bx, \\bb, \\bW)$ for each digit class $j$. In summary, the computations are done in this order:\n",
    "\n",
    "$\\bx \\rightarrow \\log \\bq \\rightarrow Z \\rightarrow \\log \\bp$\n",
    "\n",
    "Given some dataset with $N$ independent, identically distributed datapoints, the log-probability is given by:\n",
    "\n",
    "$ \\mathcal{L}(\\bb, \\bW) = \\sum_{n=1}^N \\mathcal{L}^{(n)}$\n",
    "\n",
    "where we use $\\mathcal{L}^{(n)}$ to denote the partial log-probability evaluated over a single datapoint. It is important to see that the log-probability of the class label $t^{(n)}$ given the image, is given by the $t^{(n)}$-th element of the network's output $\\log \\bp$, denoted by $\\log p_{t^{(n)}}$:\n",
    "\n",
    "$\\mathcal{L}^{(n)} = \\log p(t = t^{(n)} \\;|\\; \\bx = \\bx^{(n)}, \\bb, \\bW) = \\log p_{t^{(n)}} = \\log q_{t^{(n)}} - \\log Z^{(n)}$\n",
    "\n",
    "where $\\bx^{(n)}$ and $t^{(n)}$ are the input (image) and class label (integer) of the $n$-th datapoint, and $Z^{(n)}$ is the normalizing constant for the distribution over $t^{(n)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86f08b298588247fa91baebe97058f1b",
     "grade": false,
     "grade_id": "cell-17766ee789f11384",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Gradient-based stochastic optimization\n",
    "###  Derive gradient equations (20 points)\n",
    "\n",
    "In this section, the equations for computing the (first) partial derivatives of the log-likelihood w.r.t. all the parameters, evaluated at a _single_ datapoint $n$, will be derived\n",
    "\n",
    "We deriving the equations for $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}$ for each $j$. For clarity, we'll use the shorthand $\\delta^q_j = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}$.\n",
    "\n",
    "For $j = t^{(n)}$:\n",
    "$$\n",
    "\\delta^q_j\n",
    "= \\frac{\\partial \\log q_{t^{(n)}}}{\\partial \\log q_j}\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "= 1\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "$$\n",
    "\n",
    "For $j \\neq t^{(n)}$:\n",
    "$$\n",
    "\\delta^q_j\n",
    "= \\frac{\\partial \\log q_{t^{(n)}}}{\\partial \\log q_j}\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "=0 - \\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j}\n",
    "$$\n",
    "\n",
    "Try yourself now to complete the above derivations for $\\delta^q_j$ by furtherly developing $\\frac{\\partial \\log Z}{\\partial Z}$ and $\\frac{\\partial Z}{\\partial \\log q_j}$. Both are quite simple. For these it doesn't matter whether $j = t^{(n)}$ or not (answer will follow in next cell).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b25efc93bb5297bfd79e7dbeda7baa2",
     "grade": false,
     "grade_id": "cell-e40110444a1e1d3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "For $j = t^{(n)}$:\n",
    "\\begin{align}\n",
    "\\delta^q_j\n",
    "&= 1 - \\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \\\\\n",
    "&= 1 - \\frac{1}{Z} \\frac{\\partial \\sum_i \\exp (\\log (q_i))}{\\partial \\log(q_j)} \\\\\n",
    "&= 1 - \\frac{1}{Z} \\exp (\\log (q_j)) \\\\\n",
    "&= 1 - \\frac{q_j}{Z} \n",
    "\\end{align}\n",
    "For $j \\neq t^{(n)}$:\n",
    "\\begin{align}\n",
    "\\delta^q_j\n",
    "&= - \\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \\\\\n",
    "&= - \\frac{q_j}{Z}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ca2b3483f10327000769ddd6432fb6f",
     "grade": false,
     "grade_id": "cell-c770cfe1389ca4ff",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Given the equations for computing the gradients $\\delta^q_j$ it is quite straightforward to derive the equations for the gradients of the parameters of the model, $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}}$ and $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial b_j}$. The gradients for the biases $\\bb$ are given by:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial b_j}\n",
    "= \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial b_j}\n",
    "= \\delta^q_j\n",
    "\\cdot 1\n",
    "= \\delta^q_j\n",
    "$\n",
    "\n",
    "The equation above gives the derivative of $\\mathcal{L}^{(n)}$ w.r.t. a single element of $\\bb$, so the vector $\\nabla_\\bb \\mathcal{L}^{(n)}$ with all derivatives of $\\mathcal{L}^{(n)}$ w.r.t. the bias parameters $\\bb$ is: \n",
    "\n",
    "$\n",
    "\\nabla_\\bb \\mathcal{L}^{(n)} = \\mathbf{\\delta}^q\n",
    "$\n",
    "\n",
    "where $\\mathbf{\\delta}^q$ denotes the vector of size $10 \\times 1$ with elements $\\mathbf{\\delta}_j^q$.\n",
    "\n",
    "The (not fully developed) equation for computing the derivative of $\\mathcal{L}^{(n)}$ w.r.t. a single element $W_{ij}$ of $\\bW$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "$\n",
    "\n",
    "Try again yourself: what is $\\frac{\\partial \\log q_j}{\\partial W_{ij}}$? If you want, you can give the resulting equation in vector format ($\\nabla_{\\bw_j} \\mathcal{L}^{(n)} = ...$), like we did for $\\nabla_\\bb \\mathcal{L}^{(n)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d73c7fdee91b12d548d31c2dfc6a388d",
     "grade": false,
     "grade_id": "cell-e40110444a1asdfasdfd3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The answer:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}} = \\frac{\\partial }{\\partial W_{ij}}(\\textbf{w}^T_{j} \\textbf{x} + b_{j}) = \\frac{\\partial }{\\partial W_{ij}} (\\sum_i W_{ij}x_i +b_j) = x_i\n",
    "$\n",
    "\n",
    "So \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =  \\mathbf{\\delta}_j^q x_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "519ed8b3a7448b29958e286105df5de9",
     "grade": false,
     "grade_id": "cell-b0f28b0924b9983d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Implement gradient computations (15 points)\n",
    "\n",
    "Implement the gradient calculations you derived in the previous question. Write a function `logreg_gradient(x, t, w, b)` that returns the gradients $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ (for each $j$) and $\\nabla_{\\bb} \\mathcal{L}^{(n)}$, i.e. the first partial derivatives of the log-likelihood w.r.t. the parameters $\\bW$ and $\\bb$, evaluated at a single datapoint (`x`, `t`).\n",
    "The computation will contain roughly the following intermediate variables:\n",
    "\n",
    "$\n",
    "\\log \\bq \\rightarrow Z \\rightarrow \\log \\bp\\,,\\, \\mathbf{\\delta}^q\n",
    "$\n",
    "\n",
    "followed by computation of the gradient vectors $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ (contained in a $784 \\times 10$ matrix) and $\\nabla_{\\bb} \\mathcal{L}^{(n)}$ (a $10 \\times 1$ vector).\n",
    "\n",
    "For maximum points, ensure the function is numerically stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e855b5e4d51374c8736346c85464595",
     "grade": false,
     "grade_id": "cell-6858f885be587480",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.1.2 Compute gradient of log p(t|x;w,b) wrt w and b\n",
    "def logreg_gradient(x, t, w, b):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # here the statement contains logp[:,t] where logp is meant tas a matrix of shape 1x10\n",
    "    \n",
    "    return logp[:,t].squeeze(), dL_dw, dL_db.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d831c5032df171bdb79da06ed44d0df",
     "grade": true,
     "grade_id": "cell-48057487182fe951",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a217db3d2929ceeaf4efae277c93da5a",
     "grade": true,
     "grade_id": "cell-1c9659f607b151a2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "# scalar, 10 X 768  matrix, 10 X 1 vector\n",
    "w = np.random.normal(size=(28*28,10), scale=0.001)\n",
    "# w = np.zeros((784,10))\n",
    "b = np.zeros((10,))\n",
    "\n",
    "# test gradients, train on 1 sample\n",
    "logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "\n",
    "print(\"Test gradient on one point\")\n",
    "print(\"Log Likelihood:\\t\", logpt)\n",
    "print(\"\\nGrad_W_ij\\t\",grad_w.shape,\"matrix\")\n",
    "print(\"Grad_W_ij[0,152:158]=\\t\", grad_w[152:158,0])\n",
    "print(\"\\nGrad_B_i shape\\t\",grad_b.shape,\"vector\")\n",
    "print(\"Grad_B_i=\\t\", grad_b.T)\n",
    "print(\"i in {0,...,9}; j in M\")\n",
    "\n",
    "assert logpt.shape == (), logpt.shape\n",
    "assert grad_w.shape == (784, 10), grad_w.shape\n",
    "assert grad_b.shape == (10,), grad_b.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af23a71e0ab14cb7d4228aae2cdd4479",
     "grade": true,
     "grade_id": "cell-fd59c3a03a87ab83",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# It's always good to check your gradient implementations with finite difference checking:\n",
    "# Scipy provides the check_grad function, which requires flat input variables.\n",
    "# So we write two helper functions that provide the gradient and output with 'flat' weights:\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "np.random.seed(123)\n",
    "# scalar, 10 X 768  matrix, 10 X 1 vector\n",
    "w = np.random.normal(size=(28*28,10), scale=0.001)\n",
    "# w = np.zeros((784,10))\n",
    "b = np.zeros((10,))\n",
    "\n",
    "def func(w):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w.reshape(784,10), b)\n",
    "    return logpt\n",
    "def grad(w):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w.reshape(784,10), b)\n",
    "    return grad_w.flatten()\n",
    "finite_diff_error = check_grad(func, grad, w.flatten())\n",
    "print('Finite difference error grad_w:', finite_diff_error)\n",
    "assert finite_diff_error < 1e-3, 'Your gradient computation for w seems off'\n",
    "\n",
    "def func(b):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "    return logpt\n",
    "def grad(b):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "    return grad_b.flatten()\n",
    "finite_diff_error = check_grad(func, grad, b)\n",
    "print('Finite difference error grad_b:', finite_diff_error)\n",
    "assert finite_diff_error < 1e-3, 'Your gradient computation for b seems off'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3db4893511a787c7d2412e1e3b7c441b",
     "grade": true,
     "grade_id": "cell-91b8c5eb86f6a0f3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# It contains hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9d3aaec15fcf8700a5a951e778cc1b8",
     "grade": false,
     "grade_id": "cell-bdce061b39aaacec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Stochastic gradient descent (15 points)\n",
    "\n",
    "Write a function `sgd_iter(x_train, t_train, w, b)` that performs one iteration of stochastic gradient descent (SGD), and returns the new weights. It should go through the trainingset once in randomized order, call `logreg_gradient(x, t, w, b)` for each datapoint to get the gradients, and update the parameters **using a small learning rate of `1e-4`**. Note that in this case we're maximizing the likelihood function, so we should actually performing gradient ___ascent___... For more information about SGD, see Bishop 5.2.4 or an online source (i.e. https://en.wikipedia.org/wiki/Stochastic_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da9d7071416585e49a45bcda21446a15",
     "grade": false,
     "grade_id": "cell-86bf84658f1c5bc8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sgd_iter(x_train, t_train, W, b):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return logp_train, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c7d07c826c36ec3071dc2268cf34dea",
     "grade": true,
     "grade_id": "cell-0929d502114babdb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed33b18ecd8a74213ebbedf0bac36096",
     "grade": true,
     "grade_id": "cell-2f7bbc264cc887a0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "np.random.seed(1243)\n",
    "w = np.zeros((28*28, 10))\n",
    "b = np.zeros(10)\n",
    "    \n",
    "logp_train, W, b = sgd_iter(x_train[:5], t_train[:5], w, b)\n",
    "\n",
    "\n",
    "print (logp_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3458140d661c7cccaa820b8db3e1864",
     "grade": false,
     "grade_id": "cell-81634c804e1f93fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Train\n",
    "\n",
    "### Train (15 points)\n",
    "Perform SGD on the training set. Plot (in one graph) the conditional log-probability of the training set and validation set after each iteration. (6 points)\n",
    "\n",
    "Instead of running SGD for a fixed number of steps, run it until convergence. Think of a reasonable criterion for determining convergence. As a reference: choose a criterion such that the algorithm terminates in less than 15 iterations over the training set. (2 points)\n",
    "\n",
    "Make sure your implementation (in particular, the output of the conditional log-probability of the training set and validation set) is independent of the size of the dataset. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e06c63969e74ed5a44a080a093f89c85",
     "grade": false,
     "grade_id": "cell-20a347ba4db6e82c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_sgd(x_train, t_train, x_valid, t_valid, w, b):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "np.random.seed(1243)\n",
    "w = np.zeros((28*28, 10))\n",
    "b = np.zeros(10)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "w,b = test_sgd(x_train, t_train, x_valid, t_valid, w, b)\n",
    "end = time.time()\n",
    "ellapsed_time = end - start\n",
    "\n",
    "print (ellapsed_time)\n",
    "assert np.allclose (ellapsed_time, 36.28032302856445, atol=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a10ca977cd4a8a58e57ed81c8d2dc7f5",
     "grade": true,
     "grade_id": "cell-b290fe89d0aa4ffb",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# Hidden tests for efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a311dcfc8324211852be9281fc5b52dd",
     "grade": false,
     "grade_id": "cell-cf7f3da57d19493a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Visualize weights\n",
    "As a sanity check, Visualize the resulting parameters $\\bW$ after a few iterations through the training set, by treating each column of $\\bW$ as an image. If you want, you can use or edit the `plot_digits(...)` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37f4f49d867dfb6ea41144da5f8c06fe",
     "grade": false,
     "grade_id": "cell-b10656f35fac065e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_digits(w.T, num_cols=5, targets=np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a82f8a1708c23ebcd19bd61862a521",
     "grade": false,
     "grade_id": "cell-eb131c8b7303da38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Food for thoughL Think of reasons that these weights are minimizing your loss**\n",
    "Note, you dont need to provide any answer in this question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731a9bcd86097e41432f62771b8d3973",
     "grade": false,
     "grade_id": "cell-f36d974d9ef34c97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize the 8 hardest and 8 easiest digits\n",
    "Visualize the 8 digits in the validation set with the highest probability of the true class label under the model.\n",
    "Also plot the 8 digits that were assigned the lowest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67ff6509c6c13336cae6aaf806e16f6e",
     "grade": false,
     "grade_id": "cell-3802d61680deeff5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logreg_logp(x, t, w, b):\n",
    "    logq = np.dot(x, w) + b\n",
    "    amax = np.max(logq, axis=-1, keepdims=True)\n",
    "    logz = amax + np.log(np.sum(np.exp(logq - amax), axis=-1, keepdims=True))\n",
    "    return logq[np.arange(len(t)), t] - logz.squeeze()\n",
    "\n",
    "N = 8\n",
    "logp_valid = logreg_logp(x_valid, t_valid, w, b)\n",
    "easiest = np.argpartition(logp_valid, -N)[-N:]\n",
    "hardest = np.argpartition(-logp_valid, -N)[-N:]\n",
    "\n",
    "plot_digits(x_valid[easiest], num_cols=4)\n",
    "plot_digits(x_valid[hardest], num_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8347912a552441798c48496bb70b647f",
     "grade": false,
     "grade_id": "cell-6564a51fdda06d95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Food for though: Ask yourself if these results make sense.**\n",
    "You do not need to provide any answer on this question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "337becf9b9af98a4fc4c67cf618575dc",
     "grade": false,
     "grade_id": "cell-5a22aa21945e12a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2. Principal Component Analysis (PCA) on MNIST (25 points)\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical method aiming to transform the original variables into a new set of uncorrelated variables (principal components) that represent most of the variability in the dataset. The principal components are linear combinations of the original variables, defined as:\n",
    "\n",
    "$$Y_i = a_{i1}X_1 + a_{i2}X_2 + \\ldots + a_{ip}X_p $$\n",
    "\n",
    "### Applying PCA to MNIST\n",
    "\n",
    "1. **Data Preparation:** use the loaded MNIST dataset from the part 1 of the lab.\n",
    "<br>\n",
    "2. **Implementing PCA Function:** \n",
    "    - **Center the Data:** Subtract the mean of each feature, resulting in the mean-centered data $\\bar{X}$:\n",
    "    \n",
    "    $$ \\bar{X} = X - \\mu_X $$\n",
    "    \n",
    "    where $\\ \\mu_X$  the mean of the data matrix $ X $ (NxD) calculated column-wise.\n",
    "    \n",
    "    - **Calculate Covariance Matrix:** Compute the covariance matrix $C$ of the mean-centered data $\\bar{X}$. The covariance matrix is given by:\n",
    "    \n",
    "    $$ C = \\frac{1}{N} \\bar{X}^T \\bar{X} $$\n",
    "    \n",
    "    - **Compute Eigenvalues and Eigenvectors:** Derive the eigenvalues and eigenvectors of the covariance matrix $C$. Each eigenvector represents a principal component, and the corresponding eigenvalue indicates the variance explained by that principal component.\n",
    "<br>\n",
    "\n",
    "3. **Choosing the Number of Components:** \n",
    "    - **Sort Eigenvalues:** Order the eigenvalues in descending order.\n",
    "    \n",
    "    $$ \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_D $$\n",
    "    \n",
    "    - **Select Components:** Choose the smallest number $k$ such that the following holds:\n",
    "    \n",
    "    $$ \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{D} \\lambda_i} \\geq r $$\n",
    "    \n",
    "    where $r$ is the desired proportion of total variance to preserve.\n",
    "<br>\n",
    "\n",
    "4. **Data Transformation:** Use the original data $X$ and the chosen principal components to obtain the transformed data $Y$:\n",
    "    \n",
    "    $$ Y = X P_k $$\n",
    "    \n",
    "    where $P_k$ is the matrix with the first $k$ eigenvectors as columns.\n",
    "<br>\n",
    "\n",
    "5. **Reconstructing and Visualizing the Data:** Reconstruct the original data from the transformed data $Y$ by using the inverse transformation:\n",
    "    \n",
    "    $$ X_{\\text{reconstructed}} = Y P_k^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:41:17.178101770Z",
     "start_time": "2023-09-26T12:41:17.137835320Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "863506d43c565a1dec74f2f97c9a8eb0",
     "grade": false,
     "grade_id": "cell-ff27c3ded0c983d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find eigenvecs/eigenvals of the centered covariance matrix. You can use np.linalg.eig to find the eigenvalues and eigenvectors.\n",
    "# Args:\n",
    "#     data (numpy.ndarray): The dataset to apply PCA on.\n",
    "# Returns:\n",
    "#     Tuple of:\n",
    "#     - eigenvalues (numpy.ndarray): Eigenvalues representing variance explained by each component.\n",
    "#     - eigenvectors (numpy.ndarray): Eigenvectors as directions of principal components.\n",
    "\n",
    "def PCA(data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:48:22.955160907Z",
     "start_time": "2023-09-26T12:48:22.872999529Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42b909a3f75000c6c2376add74aed10d",
     "grade": false,
     "grade_id": "cell-33b3b86ab61c4f7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Select the optimal number of principal components based on a desired variance percentage. Don't forget to sort the eigenvalues/eigenvectors!\n",
    "# Args:\n",
    "#     eigenvalues (numpy.ndarray): Eigenvalues representing variance explained by each component.\n",
    "#     eigenvectors (numpy.ndarray): Eigenvectors as directions of principal components.\n",
    "#     percentage (float): Desired proportion of total variance to preserve.\n",
    "# Returns:\n",
    "#     selected_components (numpy.ndarray): Selected principal components that satisfy the explained variance threshold.\n",
    "\n",
    "def select_components(eigenvalues, eigenvectors, percentage):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:48:23.477553542Z",
     "start_time": "2023-09-26T12:48:23.401751993Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59d1108dc91c33d7bb0d87f6199e7b65",
     "grade": false,
     "grade_id": "cell-f6599dc0bca20949",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data using selected principal components.\n",
    "# Args:\n",
    "#     data (numpy.ndarray): The data we wish to transform.\n",
    "#     selected_components (numpy.ndarray): Selected principal components for transformation.\n",
    "# Returns:\n",
    "#     transformed_data (numpy.ndarray): Data transformed into a lower-dimensional space using PCA.\n",
    "\n",
    "def transform_data(data, selected_components):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:48:23.960939331Z",
     "start_time": "2023-09-26T12:48:23.954823995Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5227dbe7908f1ac3db7c2671d8971ee",
     "grade": false,
     "grade_id": "cell-50b476934315f542",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruct the original data from transformed data using selected principal components.\n",
    "# Args:\n",
    "#     transformed_data (numpy.ndarray): Data transformed into a lower-dimensional space using PCA.\n",
    "#     selected_components (numpy.ndarray): Selected principal components used for transformation.\n",
    "# Returns:\n",
    "#     reconstructed_data (numpy.ndarray): Data reconstructed from the lower-dimensional representation.\n",
    "\n",
    "def reconstruct_data(transformed_data, selected_components):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T12:52:02.087776127Z",
     "start_time": "2023-09-26T12:52:00.155406585Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa4928aae99b779d36a4c460182401b3",
     "grade": true,
     "grade_id": "cell-a949cc4a7b4a2da6",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming data and targets are already loaded\n",
    "# data: The original MNIST dataset\n",
    "# targets: The corresponding labels\n",
    "np.random.seed(123)\n",
    "\n",
    "data = x_train\n",
    "targets = t_train\n",
    "\n",
    "# 1. Perform PCA to obtain eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = PCA(data)\n",
    "\n",
    "\n",
    "# 2. Select the top k principal components to preserve r of total variance\n",
    "r = 0.99\n",
    "selected_components = select_components(eigenvalues, eigenvectors, r)\n",
    "\n",
    "\n",
    "# 3. Transform the data using the selected principal components\n",
    "transformed_data = transform_data(data, selected_components)\n",
    "\n",
    "# 4. Reconstruct the data back to the original space\n",
    "reconstructed_data = reconstruct_data(transformed_data, selected_components)\n",
    "\n",
    "\n",
    "# Plot the original images using the plotting function\n",
    "print(\"Original Images:\")\n",
    "plot_digits(data[0:40000:5000], num_cols=4, targets=targets[0:40000:5000])\n",
    "\n",
    "# Plot the reconstructed images\n",
    "k = selected_components.shape[1]  # Number of top principal components retained\n",
    "K = 784\n",
    "print(f\"Reconstructed Images using Top {k} out of {K} Principal Components:\")\n",
    "plot_digits(reconstructed_data[0:40000:5000], num_cols=4, targets=targets[0:40000:5000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
